{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_vector_store = InMemoryVectorStore(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id}\n",
    "    )\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "\n",
    "    def _filter_function(doc: Document) -> bool:\n",
    "        return doc.metadata.get(\"user_id\") == user_id\n",
    "\n",
    "    documents = recall_vector_store.similarity_search(\n",
    "        query, k=3, filter=_filter_function\n",
    "    )\n",
    "    return [document.page_content for document in documents]\n",
    "\n",
    "search = TavilySearchResults(max_results=1)\n",
    "\n",
    "tools = [save_recall_memory, search_recall_memories, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Search for relevant memories.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_recall_memories.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define state, nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    # add memories that will be retrieved based on the conversation context\n",
    "    recall_memories: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant with advanced long-term memory\"\n",
    "            \" capabilities. Powered by a stateless LLM, you must rely on\"\n",
    "            \" external memory to store information between conversations.\"\n",
    "            \" Utilize the available memory tools to store and retrieve\"\n",
    "            \" important details that will help you better attend to the user's\"\n",
    "            \" needs and understand their context.\\n\\n\"\n",
    "\n",
    "            \"Memory Usage Guidelines:\\n\"\n",
    "            \"1. Actively use memory tools (save_core_memory, save_recall_memory)\"\n",
    "            \" to build a comprehensive understanding of the user.\\n\"\n",
    "            \"2. Make informed suppositions and extrapolations based on stored\"\n",
    "            \" memories.\\n\"\n",
    "            \"3. Regularly reflect on past interactions to identify patterns and\"\n",
    "            \" preferences.\\n\"\n",
    "            \"4. Update your mental model of the user with each new piece of\"\n",
    "            \" information.\\n\"\n",
    "            \"5. Cross-reference new information with existing memories for\"\n",
    "            \" consistency.\\n\"\n",
    "            \"6. Prioritize storing emotional context and personal values\"\n",
    "            \" alongside facts.\\n\"\n",
    "            \"7. Use memory to anticipate needs and tailor responses to the\"\n",
    "            \" user's style.\\n\"\n",
    "            \"8. Recognize and acknowledge changes in the user's situation or\"\n",
    "            \" perspectives over time.\\n\"\n",
    "            \"9. Leverage memories to provide personalized examples and\"\n",
    "            \" analogies.\\n\"\n",
    "            \"10. Recall past challenges or successes to inform current\"\n",
    "            \" problem-solving.\\n\\n\"\n",
    "\n",
    "            \"## Recall Memories\\n\"\n",
    "            \"Recall memories are contextually retrieved based on the current\"\n",
    "            \" conversation:\\n{recall_memories}\\n\\n\"\n",
    "            \n",
    "            \"## Instructions\\n\"\n",
    "            \"Engage with the user naturally, as a trusted colleague or friend.\"\n",
    "            \" There's no need to explicitly mention your memory capabilities.\"\n",
    "            \" Instead, seamlessly incorporate your understanding of the user\"\n",
    "            \" into your responses. Be attentive to subtle cues and underlying\"\n",
    "            \" emotions. Adapt your communication style to match the user's\"\n",
    "            \" preferences and current emotional state. Use tools to persist\"\n",
    "            \" information you want to retain in the next conversation. If you\"\n",
    "            \" do call tools, all text preceding the tool call is an internal\"\n",
    "            \" message. Respond AFTER calling the tool, once you have\"\n",
    "            \" confirmation that the tool completed successfully.\\n\\n\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | model_with_tools\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"recall_memories\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    convo_str = get_buffer_string(state[\"messages\"])\n",
    "    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])\n",
    "    recall_memories = search_recall_memories.invoke(convo_str, config)\n",
    "    return {\n",
    "        \"recall_memories\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e82a0e0610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the graph and add nodes\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(load_memories)\n",
    "builder.add_node(agent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e82a0e0610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add edges to the graph\n",
    "builder.add_edge(START, \"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MemorySaver = InMemorySaver  # Kept for backwards compatibility\n",
    "\n",
    "In LangChain's LangGraph module, the MemorySaver class acts as a checkpointer to persist the agent's state, particularly its memory, during execution. It stores the agent's state in memory and associates it with a unique thread ID, allowing for the agent's execution to be resumed from where it left off. \n",
    "\n",
    "```python\n",
    "class InMemorySaver(\n",
    "    BaseCheckpointSaver[str], AbstractContextManager, AbstractAsyncContextManager\n",
    "):\n",
    "    \"\"\"An in-memory checkpoint saver.\n",
    "\n",
    "    This checkpoint saver stores checkpoints in memory using a defaultdict.\n",
    "\n",
    "    Note:\n",
    "        Only use `InMemorySaver` for debugging or testing purposes.\n",
    "        For production use cases we recommend installing [langgraph-checkpoint-postgres](https://pypi.org/project/langgraph-checkpoint-postgres/) and using `PostgresSaver` / `AsyncPostgresSaver`.\n",
    "\n",
    "    Args:\n",
    "        serde (Optional[SerializerProtocol]): The serializer to use for serializing and deserializing checkpoints. Defaults to None.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_stream_chunk(chunk):\n",
    "    for node, updates in chunk.items():\n",
    "        print(f\"Update from node: {node}\")\n",
    "        if \"messages\" in updates:\n",
    "            updates[\"messages\"][-1].pretty_print()\n",
    "        else:\n",
    "            print(updates)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the first session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': []}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice to meet you, John! How can I assist you today?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"messages\": [(\"user\", \"my name is John\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': []}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  save_recall_memory (call_R11WdqqZtyVJibdHvm1OPqAc)\n",
      " Call ID: call_R11WdqqZtyVJibdHvm1OPqAc\n",
      "  Args:\n",
      "    memory: John loves pizza.\n",
      "\n",
      "\n",
      "Update from node: tools\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: save_recall_memory\n",
      "\n",
      "John loves pizza.\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Pizza is a delicious choice! Do you have a favorite type of pizza or toppings you prefer?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"messages\": [(\"user\", \"i love pizza\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': ['John loves pizza.']}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  save_recall_memory (call_GqDFg6TiZJlxLlcRGceq4JCO)\n",
      " Call ID: call_GqDFg6TiZJlxLlcRGceq4JCO\n",
      "  Args:\n",
      "    memory: John's favorite pizza topping is pepperoni.\n",
      "\n",
      "\n",
      "Update from node: tools\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: save_recall_memory\n",
      "\n",
      "John's favorite pizza topping is pepperoni.\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Pepperoni is a classic favorite! It's hard to go wrong with that choice. Do you enjoy making pizza at home, or do you prefer ordering from a favorite place?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"yes -- pepperoni!\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}},\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': ['John loves pizza.', \"John's favorite pizza topping is pepperoni.\"]}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  save_recall_memory (call_Ux2FGDkCdqDzl5Tk6952xrVc)\n",
      " Call ID: call_Ux2FGDkCdqDzl5Tk6952xrVc\n",
      "  Args:\n",
      "    memory: John recently moved to New York.\n",
      "\n",
      "\n",
      "Update from node: tools\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: save_recall_memory\n",
      "\n",
      "John recently moved to New York.\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's exciting! New York is famous for its pizza, so you're in for a treat. Have you had the chance to try any local pizzerias yet?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"i also just moved to new york\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}},\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's try out the saved information about our user on a different thread:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': ['John loves pizza.', 'John recently moved to New York.', \"John's favorite pizza topping is pepperoni.\"]}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you recently moved to New York and love pizza, have you thought about trying out one of the city's iconic pizza spots? There are plenty of renowned pizza places, and since you love pepperoni, you might enjoy a classic New York-style pepperoni slice. Some popular pizzerias include Joe's Pizza, Lombardi's, or Di Fara Pizza. Would you like more detailed options or recommendations in a different cuisine?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"2\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"where should i go for dinner?\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adding structured memories**\n",
    "\n",
    "Below, we update the save_recall_memory tool to accept a list of \"knowledge triples\", or 3-tuples with a subject, predicate, and object, suitable for storage in a knolwedge graph. Our model will then generate these representations as part of its tool calls.\n",
    "\n",
    "For simplicity, we use the same vector database as before, but the save_recall_memory and search_recall_memories tools could be further updated to interact with a graph database. For now, we only need to update the save_recall_memory tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_vector_store = InMemoryVectorStore(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class KnowledgeTriple(TypedDict):\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object_: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def save_recall_memory(memories: List[KnowledgeTriple], config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    for memory in memories:\n",
    "        serialized = \" \".join(memory.values())\n",
    "        document = Document(\n",
    "            serialized,\n",
    "            id=str(uuid.uuid4()),\n",
    "            metadata={\n",
    "                \"user_id\": user_id,\n",
    "                **memory,\n",
    "            },\n",
    "        )\n",
    "        recall_vector_store.add_documents([document])\n",
    "    return memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [save_recall_memory, search_recall_memories, search]\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e82b8b2f50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the graph and add nodes\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(load_memories)\n",
    "builder.add_node(agent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e82b8b2f50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add edges to the graph\n",
    "builder.add_edge(START, \"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First interaction after adding the graph database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"3\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"Hi, I'm Alice.\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"My friend John likes Pizza.\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"3\", \"thread_id\": \"2\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"What food should I bring to John's party?\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, for illustrative purposes we can visualize the knowledge graph extracted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Fetch records\n",
    "records = recall_vector_store.similarity_search(\n",
    "    \"Alice\", k=2, filter=lambda doc: doc.metadata[\"user_id\"] == \"3\"\n",
    ")\n",
    "\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for record in records:\n",
    "    G.add_edge(\n",
    "        record.metadata[\"subject\"],\n",
    "        record.metadata[\"object_\"],\n",
    "        label=record.metadata[\"predicate\"],\n",
    "    )\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=3000,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    font_weight=\"bold\",\n",
    "    arrows=True,\n",
    ")\n",
    "edge_labels = nx.get_edge_attributes(G, \"label\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memory-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
